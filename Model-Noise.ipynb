{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$TF_KERAS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "!echo $TF_KERAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard module is not an IPython extension.\n"
     ]
    }
   ],
   "source": [
    "from keras_radam import RAdam\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "import scipy\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 14878617181860567226, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 3174592921\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10738967871256365166\n",
       " physical_device_desc: \"device: 0, name: GeForce MX150, pci bus id: 0000:01:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnTrain\\\\clnsp1.wav', 'cnTrain\\\\clnsp10.wav', 'cnTrain\\\\clnsp2.wav', 'cnTrain\\\\clnsp3.wav', 'cnTrain\\\\clnsp4.wav', 'cnTrain\\\\clnsp5.wav', 'cnTrain\\\\clnsp6.wav', 'cnTrain\\\\clnsp7.wav', 'cnTrain\\\\clnsp8.wav', 'cnTrain\\\\clnsp9.wav', 'cnTrain\\\\noisy10_SNRdb_0.0_clnsp10.wav', 'cnTrain\\\\noisy10_SNRdb_10.0_clnsp10.wav', 'cnTrain\\\\noisy10_SNRdb_20.0_clnsp10.wav', 'cnTrain\\\\noisy10_SNRdb_30.0_clnsp10.wav', 'cnTrain\\\\noisy10_SNRdb_40.0_clnsp10.wav', 'cnTrain\\\\noisy1_SNRdb_0.0_clnsp1.wav', 'cnTrain\\\\noisy1_SNRdb_10.0_clnsp1.wav', 'cnTrain\\\\noisy1_SNRdb_20.0_clnsp1.wav', 'cnTrain\\\\noisy1_SNRdb_30.0_clnsp1.wav', 'cnTrain\\\\noisy1_SNRdb_40.0_clnsp1.wav', 'cnTrain\\\\noisy2_SNRdb_0.0_clnsp2.wav', 'cnTrain\\\\noisy2_SNRdb_10.0_clnsp2.wav', 'cnTrain\\\\noisy2_SNRdb_20.0_clnsp2.wav', 'cnTrain\\\\noisy2_SNRdb_30.0_clnsp2.wav', 'cnTrain\\\\noisy2_SNRdb_40.0_clnsp2.wav', 'cnTrain\\\\noisy3_SNRdb_0.0_clnsp3.wav', 'cnTrain\\\\noisy3_SNRdb_10.0_clnsp3.wav', 'cnTrain\\\\noisy3_SNRdb_20.0_clnsp3.wav', 'cnTrain\\\\noisy3_SNRdb_30.0_clnsp3.wav', 'cnTrain\\\\noisy3_SNRdb_40.0_clnsp3.wav', 'cnTrain\\\\noisy4_SNRdb_0.0_clnsp4.wav', 'cnTrain\\\\noisy4_SNRdb_10.0_clnsp4.wav', 'cnTrain\\\\noisy4_SNRdb_20.0_clnsp4.wav', 'cnTrain\\\\noisy4_SNRdb_30.0_clnsp4.wav', 'cnTrain\\\\noisy4_SNRdb_40.0_clnsp4.wav', 'cnTrain\\\\noisy5_SNRdb_0.0_clnsp5.wav', 'cnTrain\\\\noisy5_SNRdb_10.0_clnsp5.wav', 'cnTrain\\\\noisy5_SNRdb_20.0_clnsp5.wav', 'cnTrain\\\\noisy5_SNRdb_30.0_clnsp5.wav', 'cnTrain\\\\noisy5_SNRdb_40.0_clnsp5.wav', 'cnTrain\\\\noisy6_SNRdb_0.0_clnsp6.wav', 'cnTrain\\\\noisy6_SNRdb_10.0_clnsp6.wav', 'cnTrain\\\\noisy6_SNRdb_20.0_clnsp6.wav', 'cnTrain\\\\noisy6_SNRdb_30.0_clnsp6.wav', 'cnTrain\\\\noisy6_SNRdb_40.0_clnsp6.wav', 'cnTrain\\\\noisy7_SNRdb_0.0_clnsp7.wav', 'cnTrain\\\\noisy7_SNRdb_10.0_clnsp7.wav', 'cnTrain\\\\noisy7_SNRdb_20.0_clnsp7.wav', 'cnTrain\\\\noisy7_SNRdb_30.0_clnsp7.wav', 'cnTrain\\\\noisy7_SNRdb_40.0_clnsp7.wav', 'cnTrain\\\\noisy8_SNRdb_0.0_clnsp8.wav', 'cnTrain\\\\noisy8_SNRdb_10.0_clnsp8.wav', 'cnTrain\\\\noisy8_SNRdb_20.0_clnsp8.wav', 'cnTrain\\\\noisy8_SNRdb_30.0_clnsp8.wav', 'cnTrain\\\\noisy8_SNRdb_40.0_clnsp8.wav', 'cnTrain\\\\noisy9_SNRdb_0.0_clnsp9.wav', 'cnTrain\\\\noisy9_SNRdb_10.0_clnsp9.wav', 'cnTrain\\\\noisy9_SNRdb_20.0_clnsp9.wav', 'cnTrain\\\\noisy9_SNRdb_30.0_clnsp9.wav', 'cnTrain\\\\noisy9_SNRdb_40.0_clnsp9.wav']\n"
     ]
    }
   ],
   "source": [
    "train_tfrecords_filenames = glob.glob('cnTrain/*')\n",
    "#np.random.shuffle(train_tfrecords_filenames)\n",
    "train_tfrecords_filenames = list(train_tfrecords_filenames)\n",
    "\n",
    "val_tfrecords_filenames = glob.glob('cnValid/*')\n",
    "print(train_tfrecords_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "windowLength: 256\n",
      "overlap: 64\n",
      "ffTLength: 256\n",
      "inputFs: 48000.0\n",
      "fs: 16000.0\n",
      "numFeatures: 129\n",
      "numSegments: 8\n"
     ]
    }
   ],
   "source": [
    "windowLength = 256\n",
    "overlap      = round(0.25 * windowLength) # overlap of 75%\n",
    "ffTLength    = windowLength\n",
    "inputFs      = 48e3\n",
    "fs           = 16e3\n",
    "numFeatures  = ffTLength//2 + 1\n",
    "numSegments  = 8\n",
    "print(\"windowLength:\",windowLength)\n",
    "print(\"overlap:\",overlap)\n",
    "print(\"ffTLength:\",ffTLength)\n",
    "print(\"inputFs:\",inputFs)\n",
    "print(\"fs:\",fs)\n",
    "print(\"numFeatures:\",numFeatures)\n",
    "print(\"numSegments:\",numSegments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_record_parser(record):\n",
    "    keys_to_features = {\n",
    "        \"noise_stft_phase\": tf.io.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        'noise_stft_mag_features': tf.io.FixedLenFeature([], tf.string),\n",
    "        \"clean_stft_magnitude\": tf.io.FixedLenFeature((), tf.string)\n",
    "    }\n",
    "    \n",
    "\n",
    "    features = tf.io.parse_single_example(record, keys_to_features)\n",
    "    print(\"features: \", features)\n",
    "\n",
    "    noise_stft_mag_features = tf.io.decode_raw(features['noise_stft_mag_features'], tf.float32)\n",
    "    clean_stft_magnitude = tf.io.decode_raw(features['clean_stft_magnitude'], tf.float32)\n",
    "    noise_stft_phase = tf.io.decode_raw(features['noise_stft_phase'], tf.float32)\n",
    "\n",
    "    # reshape input and annotation images\n",
    "    noise_stft_mag_features = tf.reshape(noise_stft_mag_features, (129, 8, 1), name=\"noise_stft_mag_features\")\n",
    "    clean_stft_magnitude = tf.reshape(clean_stft_magnitude, (129, 1, 1), name=\"clean_stft_magnitude\")\n",
    "    noise_stft_phase = tf.reshape(noise_stft_phase, (129,), name=\"noise_stft_phase\")\n",
    "\n",
    "    return noise_stft_mag_features, clean_stft_magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Input, LeakyReLU, Flatten, Dense, Reshape, Conv2DTranspose, BatchNormalization, Activation\n",
    "from tensorflow.keras import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  {'clean_stft_magnitude': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=() dtype=string>, 'noise_stft_mag_features': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=() dtype=string>, 'noise_stft_phase': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=() dtype=string>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset = tf.data.TFRecordDataset([train_tfrecords_filenames])\n",
    "train_dataset = train_dataset.map(tf_record_parser)\n",
    "#train_dataset = train_dataset.shuffle(5)\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(512)\n",
    "#train_dataset = train_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  {'clean_stft_magnitude': <tf.Tensor 'ParseSingleExample/ParseSingleExample:0' shape=() dtype=string>, 'noise_stft_mag_features': <tf.Tensor 'ParseSingleExample/ParseSingleExample:1' shape=() dtype=string>, 'noise_stft_phase': <tf.Tensor 'ParseSingleExample/ParseSingleExample:2' shape=() dtype=string>}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_dataset = tf.data.TFRecordDataset([val_tfrecords_filenames])\n",
    "test_dataset = test_dataset.map(tf_record_parser)\n",
    "test_dataset = test_dataset.repeat(1)\n",
    "test_dataset = test_dataset.batch(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
    "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.0006))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if use_bn:\n",
    "        x = BatchNormalization()(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_pre_activation_block(x, filters, kernel_size, strides, padding='same', use_bn=True):\n",
    "    shortcut = x\n",
    "    in_channels = x.shape[-1]\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(filters=in_channels, kernel_size=kernel_size, strides=strides, padding='same')(x)\n",
    "\n",
    "    return shortcut + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Lambda;\n",
    "\n",
    "def norm(fc2):\n",
    "\n",
    "    fc2_norm = tf.keras.backend.l2_normalize(fc2, axis = 3)\n",
    "\n",
    "    illum_est = tf.reduce_sum(fc2_norm, axis = (1, 2))\n",
    "\n",
    "    illum_est = tf.keras.backend.l2_normalize(illum_est)\n",
    "\n",
    "    return illum_est\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "def build_model(l2_strength):\n",
    "    inputs = Input(shape=(numFeatures,numSegments,1))\n",
    "    x = inputs\n",
    "    print(\"Input Shape: \", x.shape)\n",
    "  # -----\n",
    "    x = tf.keras.layers.ZeroPadding2D(((4,4), (0,0)))(x)\n",
    "    x = Conv2D(filters=18, kernel_size=[9,8], strides=[1, 1], padding='valid', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    skip0 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(skip0)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "  # -----\n",
    "    x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    skip1 = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(skip1)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    # ----\n",
    "    x = Conv2D(filters=18, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=30, kernel_size=[5,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv2D(filters=8, kernel_size=[9,1], strides=[1, 1], padding='same', use_bias=False,\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(l2_strength))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "  # ----\n",
    "    x = tf.keras.layers.SpatialDropout2D(0.2)(x)\n",
    "    x = Conv2D(filters=1, kernel_size=[129,1], strides=[1, 1], padding='same')(x)\n",
    "   # out = Lambda(norm)(x)\n",
    "    print(x.shape);\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(3e-4)\n",
    "  #optimizer = RAdam(total_steps=10000, warmup_proportion=0.1, min_lr=3e-4)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape:  (?, 129, 8, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\envs\\CNN\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\envs\\CNN\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "(?, 129, 1, 1)\n",
      "WARNING:tensorflow:From C:\\Users\\ASUS\\Anaconda3\\envs\\CNN\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 129, 8, 1)         0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 137, 8, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 129, 1, 18)        1296      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 129, 1, 18)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1 (Batc (None, 129, 1, 18)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 129, 1, 30)        2700      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 129, 1, 30)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_1 (Ba (None, 129, 1, 30)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 129, 1, 8)         2160      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 129, 1, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_2 (Ba (None, 129, 1, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 129, 1, 18)        1296      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 129, 1, 18)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_3 (Ba (None, 129, 1, 18)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 129, 1, 30)        2700      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 129, 1, 30)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_4 (Ba (None, 129, 1, 30)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 129, 1, 8)         2160      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 129, 1, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_5 (Ba (None, 129, 1, 8)         32        \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 129, 1, 18)        1296      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 129, 1, 18)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_6 (Ba (None, 129, 1, 18)        72        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 129, 1, 30)        2700      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 129, 1, 30)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_7 (Ba (None, 129, 1, 30)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 129, 1, 8)         2160      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 129, 1, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_v1_8 (Ba (None, 129, 1, 8)         32        \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d (SpatialDr (None, 129, 1, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 129, 1, 1)         1033      \n",
      "=================================================================\n",
      "Total params: 20,173\n",
      "Trainable params: 19,837\n",
      "Non-trainable params: 336\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = build_model(l2_strength=0.0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def l2_norm(vector):\n",
    "    return np.square(vector)\n",
    "\n",
    "def SDR(denoised, cleaned, eps=1e-7): # Signal to Distortion Ratio\n",
    "    a = l2_norm(denoised)\n",
    "    b = l2_norm(denoised - cleaned)\n",
    "    a_b = a / b\n",
    "    return np.mean(10 * np.log10(a_b + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hiiiiiiiiiiiiiiiiiiiiiii  <DatasetV1Adapter shapes: ((?, 129, 8, 1), (?, 129, 1, 1)), types: (tf.float32, tf.float32)>\n",
      "Epoch 1/3\n"
     ]
    }
   ],
   "source": [
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch')\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='denoiser_cnn_log_mel_generator.h5', \n",
    "                                                         monitor='val_loss', save_best_only=True)\n",
    "#print(len(tf.data.train_dataset))\n",
    "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
    "\n",
    "\n",
    "#clear_tf_session()\n",
    "print(\"hiiiiiiiiiiiiiiiiiiiiiii \",train_dataset)\n",
    "model.fit(train_dataset,\n",
    "          steps_per_epoch=2,\n",
    "          validation_data=test_dataset,\n",
    "          epochs=3,\n",
    "          callbacks=[early_stopping_callback, tensorboard_callback]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
